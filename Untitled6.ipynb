{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "890b0972-9d2d-42ed-bcc9-3df4f26fc540",
   "metadata": {},
   "outputs": [],
   "source": [
    "## best_score_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a7e86b-767a-4711-86f4-9c02b14e4d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel, AdamW, BertTokenizer, RobertaTokenizer, RobertaModel, AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "import math\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ed56f9-5215-45bf-9525-471456fa88bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_num = 22\n",
    "random.seed(seed_num)\n",
    "np.random.seed(seed_num)\n",
    "torch.manual_seed(seed_num)\n",
    "torch.cuda.manual_seed_all(seed_num)\n",
    "kf = KFold(n_splits=5, random_state=seed_num, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129863c9-be90-4b0b-b686-468e20f6874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c4c7b4-3868-4e02-b563-0448fa00251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/kor-nli/dacon/open/train_data.csv')\n",
    "test = pd.read_csv('/kaggle/input/kor-nli/dacon/open/test_data.csv')\n",
    "submission = pd.read_csv('/kaggle/input/kor-nli/dacon/open/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29864acb-1121-4bad-b79c-b7b1796ac40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"premise_\"] = \"[CLS]\" + train[\"premise\"] + \"[SEP]\"\n",
    "train[\"hypothesis_\"] = train[\"hypothesis\"] + \"[SEP]\"\n",
    "\n",
    "test[\"premise_\"] = \"[CLS]\" + test[\"premise\"] + \"[SEP]\"\n",
    "test[\"hypothesis_\"] = test[\"hypothesis\"] + \"[SEP]\"\n",
    "\n",
    "train[\"text_sum\"] = train.premise_ + \" \" + train.hypothesis_\n",
    "test[\"text_sum\"] = test.premise_ + \" \" + test.hypothesis_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20469855-58f7-43ad-916c-7125b0b2876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('klue/bert-base')\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"klue/roberta-large\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-discriminator\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2941d01b-e14a-4413-ba3c-bbf5359e18b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\"entailment\" : 0, \"contradiction\" : 1, \"neutral\" : 2}\n",
    "\n",
    "train['label'] = train['label'].apply(lambda x: label_dict[x])\n",
    "\n",
    "sen1 = train.premise[0]\n",
    "\n",
    "sen2 = train.hypothesis[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdf137c-d438-473c-98e8-3a5dc1a6350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding1(sen):\n",
    "    output = tokenizer(sen, truncation=True)\n",
    "    return output['input_ids']\n",
    "\n",
    "def encoding2(sen):\n",
    "    output = tokenizer(sen, truncation=True)\n",
    "    return output['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83236838-5c2e-47f7-adbe-5cf16f2332a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = train['text_sum'].apply(encoding1)\n",
    "att = train['text_sum'].apply(encoding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca0e5a7-83a9-4b27-b17b-9190e67e63ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_token(sen1, sen2):\n",
    "    output = tokenizer(sen1, sen2, truncation=True, padding=True, max_length=70)\n",
    "    return output['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35617e67-146b-421a-8e60-0b28ddf77b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for sen1, sen2 in zip(train.premise_, train.hypothesis_):\n",
    "    result.append(make_token(sen1, sen2))\n",
    "train['token_type'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca30f84-0cb5-4b84-a9f6-664758eb174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(sentence):\n",
    "    max_len = 70\n",
    "    l = len(sentence)\n",
    "    if l <= max_len:\n",
    "        sentence = sentence + [0] * (max_len - l)\n",
    "    else:\n",
    "        sentence = sentence[:max_len]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397018b1-86cf-4502-88e2-fb425e37201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sen1 = sen1.apply(padding)\n",
    "# att1 = att1.apply(padding)\n",
    "# sen2 = sen2.apply(padding)\n",
    "# att2 = att2.apply(padding)\n",
    "sen = sen.apply(padding)\n",
    "att = att.apply(padding)\n",
    "tok = train['token_type'].apply(padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95388ff4-1688-4ea5-a00d-d7953b13a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids1 = torch.tensor(sen1)\n",
    "# att_mask1 = torch.tensor(att1)\n",
    "# input_ids2 = torch.tensor(sen2)\n",
    "# att_mask2 = torch.tensor(att2)\n",
    "# label = torch.tensor(train['label'])\n",
    "input_ids = torch.tensor(sen)\n",
    "att_mask = torch.tensor(att)\n",
    "token_type = torch.tensor(tok)\n",
    "label = torch.tensor(train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364d304a-2fc7-4d11-aa39-5aaa1f3e6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = TensorDataset(input_ids1, att_mask1, input_ids2, att_mask2, label)\n",
    "train_dataset = TensorDataset(input_ids, att_mask, token_type, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3511e-b367-4e83-9570-c426a8457ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# train_dataLoader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6fe10b-3be5-4f97-bc4e-837294dc3823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STSClassifier(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.fc_dim = 1024\n",
    "#         self.bert_model = BertModel.from_pretrained(\"klue/bert-base\")\n",
    "        self.bert_model = AutoModel.from_pretrained('klue/roberta-large')\n",
    "        self.fc = nn.Linear(self.bert_model.config.hidden_size, self.fc_dim)\n",
    "        self.bn = nn.BatchNorm1d(self.fc_dim)\n",
    "        self.fc2=  nn.Linear(self.bert_model.config.hidden_size * 3, 3)\n",
    "        self._init_params()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "#         self.fc2 = nn.Linear(self.fc_dim, 3)\n",
    "        self.fc3 = nn.Linear(self.fc_dim, 3)\n",
    "        self.bn2 = nn.BatchNorm1d(3)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        \n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "    \n",
    "#     def forward(self, sen1, mask1, sen2, mask2, label=None):\n",
    "#         x1 = self.bert_model(input_ids=sen1, attention_mask=mask1)\n",
    "#         x1 = torch.sum(x1.last_hidden_state * mask1.unsqueeze(-1), dim=1) / mask1.sum(dim=1, keepdims=True)\n",
    "#         # x1 = self.fc(x1)\n",
    "#         # x1 = self.bn(x1)\n",
    "#         x2 = self.bert_model(input_ids=sen2, attention_mask=mask2)\n",
    "#         x2 = torch.sum(x2.last_hidden_state * mask2.unsqueeze(-1), dim=1) / mask2.sum(dim=1, keepdims=True)\n",
    "#         # x2 = self.fc(x2)\n",
    "#         # x2 = self.bn(x2)\n",
    "# #         output = torch.stack([x1, x2, abs(x1 - x2), x1*x2])\n",
    "#         output = torch.cat([x1, x2, x1-x2], dim=1)\n",
    "# #         output = output.view(-1, self.fc_dim * 4)\n",
    "#         output = self.bn2(self.fc2(self.act(output)))\n",
    "#         return output\n",
    "    def forward(self, sen, mask, token, label=None):\n",
    "        x = self.bert_model(input_ids=sen, token_type_ids=token, attention_mask=mask)\n",
    "        x = torch.sum(x.last_hidden_state * mask.unsqueeze(-1), dim=1) / mask.sum(dim=1, keepdims=True)\n",
    "#         x = self.fc(x)\n",
    "#         x = self.bn(x)\n",
    "        return self.fc3(self.dropout(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b637a50-5fcb-4f82-a806-94f827b4d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = STSClassifier(batch_size).to(device)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "model.zero_grad()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a6af05-d61f-4967-94de-f4269a9fe1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cal_accuracy(preds, labels):\n",
    "# #     pred_flat = preds>0.5\n",
    "#     pred_flat = np.argmax(preds, axis=0).flatten()\n",
    "#     labels_flat = labels\n",
    "#     return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "def cal_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348236d9-a0d6-4518-8eca-4a32041bf4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold,(train_idx,valid_idx) in enumerate(kf.split(train_dataset)):\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n",
    "    train_dataLoader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_subsampler)\n",
    "    valid_dataLoader = DataLoader(train_dataset, batch_size=batch_size, sampler=valid_subsampler)\n",
    "    best_acc = 0\n",
    "    model = STSClassifier(batch_size).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "    model.zero_grad()\n",
    "    print(f'------------fold no---------{fold + 1}----------------------')\n",
    "    for epoch_i in range(0, epochs):\n",
    "        # model.train(False)\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_accuracy = 0\n",
    "        nb_train_steps = 0\n",
    "        for batch in tqdm(train_dataLoader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            sen, att, tok, label = batch\n",
    "#             s1, m1, s2, m2, label = batch\n",
    "            outputs = model(sen, att, tok)\n",
    "#             outputs = model(s1, m1, s2, m2)\n",
    "#             outputs = model(s1, s2, m1, m2)\n",
    "            # outputs = Arcface(outputs, label)\n",
    "            # outputs = sigmoid(outputs)\n",
    "            # loss = cal_mse(outputs, label)\n",
    "#             loss = criterion(outputs.to(torch.float32), label.unsqueeze(-1).to(torch.float32))\n",
    "            loss = criterion(outputs.to(torch.float32), label.to(torch.int64))\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            logits = outputs\n",
    "#             logits = logits.detach().cpu().numpy()\n",
    "#             label = label.unsqueeze(-1).to('cpu').numpy()\n",
    "            tmp_train_accuracy = cal_accuracy(logits, label)\n",
    "            train_accuracy += tmp_train_accuracy\n",
    "            nb_train_steps += 1\n",
    "        avg_train_loss = total_loss / len(train_dataLoader)\n",
    "        print(\"\")\n",
    "        print(epoch_i + 1, \"  Average training loss: {0:.4f}\".format(avg_train_loss))\n",
    "        print(\"  Accuracy: {0:.4f}\".format(train_accuracy/(nb_train_steps)))\n",
    "        model.eval()\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "        valid_loss = 0\n",
    "        for batch in tqdm(valid_dataLoader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "#             s1, m1, s2, m2, label = batch\n",
    "            sen, att, tok, label = batch\n",
    "            with torch.no_grad():     \n",
    "#                 outputs = model(s1, m1, s2, m2)\n",
    "                outputs = model(sen, att, tok)\n",
    "            # loss = cal_mse(outputs, label)\n",
    "            # outputs = Arcface(outputs, label)\n",
    "            # outputs = sigmoid(outputs)\n",
    "            # print(outputs)\n",
    "#             loss = criterion(outputs.to(torch.float32), label.to(torch.float32))\n",
    "#             loss = criterion(outputs.to(torch.float32), label.unsqueeze(-1).to(torch.float32))\n",
    "            loss = criterion(outputs.to(torch.float32), label.to(torch.int64))\n",
    "            valid_loss += loss.item()\n",
    "            logits = outputs\n",
    "#             logits = logits.detach().cpu().numpy()\n",
    "#             label = label.unsqueeze(-1).to('cpu').numpy()\n",
    "            tmp_eval_accuracy = cal_accuracy(logits, label)\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_steps += 1\n",
    "        avg_valid_loss = valid_loss / len(valid_dataLoader)\n",
    "        valid_accuracy = eval_accuracy/(nb_eval_steps)\n",
    "#         if avg_valid_loss <= best_loss:\n",
    "        if best_acc <= valid_accuracy:\n",
    "            best_acc = valid_accuracy\n",
    "#             best_loss = avg_valid_loss\n",
    "            torch.save(model, f'/kaggle/working/model{fold + 1}')\n",
    "            print(f'model{fold + 1} saved')\n",
    "        print(epoch_i + 1, \"  Average valid loss: {0:.4f}\".format(avg_valid_loss))\n",
    "        print(\"  Accuracy: {0:.4f}\".format(valid_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e64d3-8e1a-4908-857b-e66761e2849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## new best 0.87\n",
    "\n",
    "for fold,(train_idx,valid_idx) in enumerate(kf.split(train_dataset)):\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n",
    "    train_dataLoader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_subsampler)\n",
    "    valid_dataLoader = DataLoader(train_dataset, batch_size=batch_size, sampler=valid_subsampler)\n",
    "    best_acc = 0\n",
    "    model = STSClassifier(batch_size).to(device)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
    "    model.zero_grad()\n",
    "    print(f'------------fold no---------{fold + 1}----------------------')\n",
    "    for epoch_i in range(0, epochs):\n",
    "        # model.train(False)\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_accuracy = 0\n",
    "        nb_train_steps = 0\n",
    "        for batch in tqdm(train_dataLoader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            sen, att, tok, label = batch\n",
    "#             s1, m1, s2, m2, label = batch\n",
    "            outputs = model(sen, att, tok)\n",
    "#             outputs = model(s1, m1, s2, m2)\n",
    "#             outputs = model(s1, s2, m1, m2)\n",
    "            # outputs = Arcface(outputs, label)\n",
    "            # outputs = sigmoid(outputs)\n",
    "            # loss = cal_mse(outputs, label)\n",
    "#             loss = criterion(outputs.to(torch.float32), label.unsqueeze(-1).to(torch.float32))\n",
    "            loss = criterion(outputs.to(torch.float32), label.to(torch.int64))\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            logits = outputs\n",
    "#             logits = logits.detach().cpu().numpy()\n",
    "#             label = label.unsqueeze(-1).to('cpu').numpy()\n",
    "            tmp_train_accuracy = cal_accuracy(logits, label)\n",
    "            train_accuracy += tmp_train_accuracy\n",
    "            nb_train_steps += 1\n",
    "        avg_train_loss = total_loss / len(train_dataLoader)\n",
    "        print(\"\")\n",
    "        print(epoch_i + 1, \"  Average training loss: {0:.4f}\".format(avg_train_loss))\n",
    "        print(\"  Accuracy: {0:.4f}\".format(train_accuracy/(nb_train_steps)))\n",
    "        model.eval()\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "        valid_loss = 0\n",
    "        for batch in tqdm(valid_dataLoader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "#             s1, m1, s2, m2, label = batch\n",
    "            sen, att, tok, label = batch\n",
    "            with torch.no_grad():     \n",
    "#                 outputs = model(s1, m1, s2, m2)\n",
    "                outputs = model(sen, att, tok)\n",
    "            # loss = cal_mse(outputs, label)\n",
    "            # outputs = Arcface(outputs, label)\n",
    "            # outputs = sigmoid(outputs)\n",
    "            # print(outputs)\n",
    "#             loss = criterion(outputs.to(torch.float32), label.to(torch.float32))\n",
    "#             loss = criterion(outputs.to(torch.float32), label.unsqueeze(-1).to(torch.float32))\n",
    "            loss = criterion(outputs.to(torch.float32), label.to(torch.int64))\n",
    "            valid_loss += loss.item()\n",
    "            logits = outputs\n",
    "#             logits = logits.detach().cpu().numpy()\n",
    "#             label = label.unsqueeze(-1).to('cpu').numpy()\n",
    "            tmp_eval_accuracy = cal_accuracy(logits, label)\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_steps += 1\n",
    "        avg_valid_loss = valid_loss / len(valid_dataLoader)\n",
    "        valid_accuracy = eval_accuracy/(nb_eval_steps)\n",
    "#         if avg_valid_loss <= best_loss:\n",
    "        if best_acc <= valid_accuracy:\n",
    "            best_acc = valid_accuracy\n",
    "#             best_loss = avg_valid_loss\n",
    "            torch.save(model, f'/kaggle/working/model{fold + 1}')\n",
    "            print(f'model{fold + 1} saved')\n",
    "        print(epoch_i + 1, \"  Average valid loss: {0:.4f}\".format(avg_valid_loss))\n",
    "        print(\"  Accuracy: {0:.4f}\".format(valid_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
