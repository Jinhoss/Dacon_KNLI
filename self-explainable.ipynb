{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom transformers import BertModel, AdamW, BertTokenizer, RobertaTokenizer, RobertaModel, AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler\nfrom sklearn.model_selection import train_test_split\nimport random\nimport os\nfrom tqdm import tqdm\nfrom transformers import get_linear_schedule_with_warmup\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nimport math\nfrom torch.optim import Adam\nfrom sklearn.model_selection import KFold\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-07T20:42:03.137117Z","iopub.execute_input":"2022-02-07T20:42:03.137514Z","iopub.status.idle":"2022-02-07T20:42:10.676219Z","shell.execute_reply.started":"2022-02-07T20:42:03.1374Z","shell.execute_reply":"2022-02-07T20:42:10.675476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_num = 22\nrandom.seed(seed_num)\nnp.random.seed(seed_num)\ntorch.manual_seed(seed_num)\ntorch.cuda.manual_seed_all(seed_num)\nkf = KFold(n_splits=5, random_state=seed_num, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:10.678095Z","iopub.execute_input":"2022-02-07T20:42:10.678889Z","iopub.status.idle":"2022-02-07T20:42:10.689615Z","shell.execute_reply.started":"2022-02-07T20:42:10.678843Z","shell.execute_reply":"2022-02-07T20:42:10.686889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():    \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\nelse:\n    device = torch.device(\"cpu\")\n    print('No GPU available, using the CPU instead.')","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:10.690758Z","iopub.execute_input":"2022-02-07T20:42:10.691159Z","iopub.status.idle":"2022-02-07T20:42:10.750726Z","shell.execute_reply.started":"2022-02-07T20:42:10.691118Z","shell.execute_reply":"2022-02-07T20:42:10.749777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/kor-nli/dacon/open/train_data.csv')\ntest = pd.read_csv('/kaggle/input/kor-nli/dacon/open/test_data.csv')\nsubmission = pd.read_csv('/kaggle/input/kor-nli/dacon/open/sample_submission.csv')\n# trans = pd.read_csv('/kaggle/input/translate/trans_info.csv')\n# softlabel = pd.read_csv('/kaggle/input/softlabel/softlabel.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:10.752077Z","iopub.execute_input":"2022-02-07T20:42:10.752844Z","iopub.status.idle":"2022-02-07T20:42:10.934982Z","shell.execute_reply.started":"2022-02-07T20:42:10.7528Z","shell.execute_reply":"2022-02-07T20:42:10.934172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trans['premise'] = translate['trans_premise']\n# trans['hypothesis'] = translate['trans_hypothesis']","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:10.93886Z","iopub.execute_input":"2022-02-07T20:42:10.939096Z","iopub.status.idle":"2022-02-07T20:42:10.944801Z","shell.execute_reply.started":"2022-02-07T20:42:10.939068Z","shell.execute_reply":"2022-02-07T20:42:10.94402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"premise_\"] = \"[CLS]\" + train[\"premise\"] + \"[SEP]\"\ntrain[\"hypothesis_\"] = train[\"hypothesis\"] + \"[SEP]\"\n\ntest[\"premise_\"] = \"[CLS]\" + test[\"premise\"] + \"[SEP]\"\ntest[\"hypothesis_\"] = test[\"hypothesis\"] + \"[SEP]\"\n\ntrain[\"text_sum\"] = train.premise_ + \" \" + train.hypothesis_\ntest[\"text_sum\"] = test.premise_ + \" \" + test.hypothesis_","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:10.946151Z","iopub.execute_input":"2022-02-07T20:42:10.946798Z","iopub.status.idle":"2022-02-07T20:42:10.993599Z","shell.execute_reply.started":"2022-02-07T20:42:10.946762Z","shell.execute_reply":"2022-02-07T20:42:10.992844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train[\"premise2_\"] = \"[CLS]\" + trans[\"premise\"] + \"[SEP]\"\n# train[\"hypothesis2_\"] = train[\"hypothesis\"] + \"[SEP]\"\n# train['text_sum2'] = train.premise2_ + \" \" + train.hypothesis2_","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:10.995604Z","iopub.execute_input":"2022-02-07T20:42:10.996021Z","iopub.status.idle":"2022-02-07T20:42:11.000037Z","shell.execute_reply.started":"2022-02-07T20:42:10.995983Z","shell.execute_reply":"2022-02-07T20:42:10.999028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizer = BertTokenizer.from_pretrained('klue/bert-base')\n# tokenizer = BertTokenizer.from_pretrained(\"klue/roberta-large\")\n# tokenizer2 = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-discriminator\")\ntokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:11.001371Z","iopub.execute_input":"2022-02-07T20:42:11.001829Z","iopub.status.idle":"2022-02-07T20:42:12.456352Z","shell.execute_reply.started":"2022-02-07T20:42:11.001789Z","shell.execute_reply":"2022-02-07T20:42:12.45567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_dict = {\"entailment\" : 0, \"contradiction\" : 1, \"neutral\" : 2}\n\ntrain['label'] = train['label'].apply(lambda x: label_dict[x])","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:12.458679Z","iopub.execute_input":"2022-02-07T20:42:12.459254Z","iopub.status.idle":"2022-02-07T20:42:12.479321Z","shell.execute_reply.started":"2022-02-07T20:42:12.459209Z","shell.execute_reply":"2022-02-07T20:42:12.478517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encoding1(sen):\n    output = tokenizer(sen, truncation=True)\n    return output['input_ids']\n\ndef encoding2(sen):\n    output = tokenizer(sen, truncation=True)\n    return output['attention_mask']\n\n# def encoding3(sen):\n#     output = tokenizer2(sen, truncation=True)\n#     return output['input_ids']\n\n# def encoding4(sen):\n#     output = tokenizer2(sen, truncation=True)\n#     return output['attention_mask']\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:12.481302Z","iopub.execute_input":"2022-02-07T20:42:12.482166Z","iopub.status.idle":"2022-02-07T20:42:12.488376Z","shell.execute_reply.started":"2022-02-07T20:42:12.482126Z","shell.execute_reply":"2022-02-07T20:42:12.487548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sen = train['text_sum'].apply(encoding1)\natt = train['text_sum'].apply(encoding2)\n# sen2 = train['text_sum'].apply(encoding3)\n# att2 = train['text_sum'].apply(encoding4)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:12.490051Z","iopub.execute_input":"2022-02-07T20:42:12.49054Z","iopub.status.idle":"2022-02-07T20:42:23.705283Z","shell.execute_reply.started":"2022-02-07T20:42:12.490507Z","shell.execute_reply":"2022-02-07T20:42:23.704532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sen2 = train['text_sum2'].apply(encoding1)\n# att2 = train['text_sum2'].apply(encoding2)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:23.706366Z","iopub.execute_input":"2022-02-07T20:42:23.706613Z","iopub.status.idle":"2022-02-07T20:42:23.712339Z","shell.execute_reply.started":"2022-02-07T20:42:23.706581Z","shell.execute_reply":"2022-02-07T20:42:23.711665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_token(sen1, sen2):\n    output = tokenizer(sen1, sen2, truncation=True, padding=True, max_length=70)\n    return output['token_type_ids']","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:23.713701Z","iopub.execute_input":"2022-02-07T20:42:23.71422Z","iopub.status.idle":"2022-02-07T20:42:23.727444Z","shell.execute_reply.started":"2022-02-07T20:42:23.714182Z","shell.execute_reply":"2022-02-07T20:42:23.726623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = []\nfor s1, s2 in zip(train.premise_, train.hypothesis_):\n    result.append(make_token(s1, s2))\ntrain['token_type'] = result","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:23.732514Z","iopub.execute_input":"2022-02-07T20:42:23.73337Z","iopub.status.idle":"2022-02-07T20:42:30.033595Z","shell.execute_reply.started":"2022-02-07T20:42:23.73334Z","shell.execute_reply":"2022-02-07T20:42:30.032873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# result = []\n# for s1, s2 in zip(train.premise2_, train.hypothesis2_):\n#     result.append(make_token(s1, s2))\n# train['token_type2'] = result","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:30.035065Z","iopub.execute_input":"2022-02-07T20:42:30.035318Z","iopub.status.idle":"2022-02-07T20:42:30.039001Z","shell.execute_reply.started":"2022-02-07T20:42:30.035284Z","shell.execute_reply":"2022-02-07T20:42:30.038181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def padding(sentence):\n    max_len = 70\n    l = len(sentence)\n    if l <= max_len:\n        sentence = sentence + [0] * (max_len - l)\n    else:\n        sentence = sentence[:max_len]\n    return sentence","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:30.040552Z","iopub.execute_input":"2022-02-07T20:42:30.04106Z","iopub.status.idle":"2022-02-07T20:42:30.049554Z","shell.execute_reply.started":"2022-02-07T20:42:30.041026Z","shell.execute_reply":"2022-02-07T20:42:30.048883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sen1 = sen1.apply(padding)\n# att1 = att1.apply(padding)\n# sen2 = sen2.apply(padding)\n# att2 = att2.apply(padding)\nsen = sen.apply(padding)\natt = att.apply(padding)\ntok = train['token_type'].apply(padding)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:30.05115Z","iopub.execute_input":"2022-02-07T20:42:30.051399Z","iopub.status.idle":"2022-02-07T20:42:30.212365Z","shell.execute_reply.started":"2022-02-07T20:42:30.051366Z","shell.execute_reply":"2022-02-07T20:42:30.211654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sen2 = sen2.apply(padding)\n# att2 = att2.apply(padding)\n# # sen2 = sen2.apply(padding)\n# # att2 = att2.apply(padding)\n# tok2 = train['token_type2'].apply(padding)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:30.213519Z","iopub.execute_input":"2022-02-07T20:42:30.213792Z","iopub.status.idle":"2022-02-07T20:42:30.217537Z","shell.execute_reply.started":"2022-02-07T20:42:30.213756Z","shell.execute_reply":"2022-02-07T20:42:30.216928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_ids1 = torch.tensor(sen1)\n# att_mask1 = torch.tensor(att1)\n# input_ids2 = torch.tensor(sen2)\n# att_mask2 = torch.tensor(att2)\n# label = torch.tensor(train['label'])\ninput_ids = torch.tensor(sen)\natt_mask = torch.tensor(att)\ntoken_type = torch.tensor(tok)\nlabel = torch.tensor(train['label'])\n# input_ids2 = torch.tensor(sen2)\n# att_mask2 = torch.tensor(att2)\n# token_type2 = torch.tensor(tok2)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:30.2187Z","iopub.execute_input":"2022-02-07T20:42:30.219101Z","iopub.status.idle":"2022-02-07T20:42:31.207853Z","shell.execute_reply.started":"2022-02-07T20:42:30.219065Z","shell.execute_reply":"2022-02-07T20:42:31.207009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_ids = torch.cat([input_ids, input_ids2], dim=0)\n# att_mask = torch.cat([att_mask, att_mask2], dim=0)\n# token_type = torch.cat([token_type, token_type2], dim=0)\n# label = torch.cat([label, label], dim=0)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:31.209437Z","iopub.execute_input":"2022-02-07T20:42:31.209699Z","iopub.status.idle":"2022-02-07T20:42:31.214568Z","shell.execute_reply.started":"2022-02-07T20:42:31.209665Z","shell.execute_reply":"2022-02-07T20:42:31.213783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x = torch.tensor(softlabel['0'])\n# y = torch.tensor(softlabel['1'])\n# z = torch.tensor(softlabel['2'])\n# soft = torch.stack([x, y, z], dim=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:31.21603Z","iopub.execute_input":"2022-02-07T20:42:31.216281Z","iopub.status.idle":"2022-02-07T20:42:31.223511Z","shell.execute_reply.started":"2022-02-07T20:42:31.216246Z","shell.execute_reply":"2022-02-07T20:42:31.222882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\n# train_dataset = TensorDataset(input_ids1, att_mask1, input_ids2, att_mask2, label)\ntrain_dataset = TensorDataset(input_ids, att_mask, token_type, label)\n# train_dataset = TensorDataset(input_ids, att_mask, token_type, label, soft)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:31.22495Z","iopub.execute_input":"2022-02-07T20:42:31.225242Z","iopub.status.idle":"2022-02-07T20:42:31.23314Z","shell.execute_reply.started":"2022-02-07T20:42:31.225208Z","shell.execute_reply":"2022-02-07T20:42:31.232402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MainClassifier(nn.Module):\n    def __init__(self, batch_size):\n        super().__init__()\n        self.fc_dim = 1024\n#         self.bert_model = BertModel.from_pretrained(\"klue/bert-base\")\n        self.bert_model = AutoModel.from_pretrained('klue/roberta-large')\n        self.fc = nn.Linear(self.bert_model.config.hidden_size, self.fc_dim)\n        self.bn = nn.BatchNorm1d(3)\n        self.fc2=  nn.Linear(self.bert_model.config.hidden_size * 3, 3)\n        self.sigmoid = torch.nn.Sigmoid()\n        self.batch_size = batch_size\n        self.dropout = nn.Dropout(p=0.5)\n#         self.fc2 = nn.Linear(self.fc_dim, 3)\n        self.fc3 = nn.Linear(self.fc_dim, 3)\n        self.bn2 = nn.BatchNorm1d(3)\n        self.act = nn.ReLU()\n#         self._init_params()\n\n        \n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc3.weight)\n        nn.init.constant_(self.fc3.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n    \n#     def forward(self, sen1, mask1, sen2, mask2, label=None):\n#         x1 = self.bert_model(input_ids=sen1, attention_mask=mask1)\n#         x1 = torch.sum(x1.last_hidden_state * mask1.unsqueeze(-1), dim=1) / mask1.sum(dim=1, keepdims=True)\n#         # x1 = self.fc(x1)\n#         # x1 = self.bn(x1)\n#         x2 = self.bert_model(input_ids=sen2, attention_mask=mask2)\n#         x2 = torch.sum(x2.last_hidden_state * mask2.unsqueeze(-1), dim=1) / mask2.sum(dim=1, keepdims=True)\n#         # x2 = self.fc(x2)\n#         # x2 = self.bn(x2)\n# #         output = torch.stack([x1, x2, abs(x1 - x2), x1*x2])\n#         output = torch.cat([x1, x2, x1-x2], dim=1)\n# #         output = output.view(-1, self.fc_dim * 4)\n#         output = self.bn2(self.fc2(self.act(output)))\n#         return output\n    def forward(self, sen, mask, token, label=None):\n        x = self.bert_model(input_ids=sen, token_type_ids=token, attention_mask=mask)\n        x = torch.sum(x.last_hidden_state * mask.unsqueeze(-1), dim=1) / mask.sum(dim=1, keepdims=True)\n#         x = self.fc(x)\n#         x = self.bn(x)\n        return self.fc3(self.dropout(x))\n#         return self.fc3(x)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:31.234327Z","iopub.execute_input":"2022-02-07T20:42:31.234764Z","iopub.status.idle":"2022-02-07T20:42:31.248067Z","shell.execute_reply.started":"2022-02-07T20:42:31.234728Z","shell.execute_reply":"2022-02-07T20:42:31.247247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def knowledge_distillation_loss(logits, labels, teacher_logits):\n#     alpha = 0.1\n#     T = 10\n\n#     student_loss = F.cross_entropy(input=logits, target=labels)\n#     distillation_loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(logits/T, dim=1), F.softmax(teacher_logits/T, dim=1)) * (T * T)\n#     total_loss =  alpha*student_loss + (1-alpha)*distillation_loss\n\n#     return total_loss","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:31.249146Z","iopub.execute_input":"2022-02-07T20:42:31.249461Z","iopub.status.idle":"2022-02-07T20:42:31.260242Z","shell.execute_reply.started":"2022-02-07T20:42:31.249424Z","shell.execute_reply":"2022-02-07T20:42:31.259459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 2\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:31.262132Z","iopub.execute_input":"2022-02-07T20:42:31.262918Z","iopub.status.idle":"2022-02-07T20:42:31.269695Z","shell.execute_reply.started":"2022-02-07T20:42:31.262893Z","shell.execute_reply":"2022-02-07T20:42:31.268893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def cal_accuracy(preds, labels):\n# #     pred_flat = preds>0.5\n#     pred_flat = np.argmax(preds, axis=0).flatten()\n#     labels_flat = labels\n#     return np.sum(pred_flat == labels_flat) / len(labels_flat)\ndef cal_accuracy(X,Y):\n    max_vals, max_indices = torch.max(X, 1)\n    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n    return train_acc","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:31.271046Z","iopub.execute_input":"2022-02-07T20:42:31.271472Z","iopub.status.idle":"2022-02-07T20:42:31.278555Z","shell.execute_reply.started":"2022-02-07T20:42:31.271427Z","shell.execute_reply":"2022-02-07T20:42:31.277848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## new best 0.87\n\nfor fold,(train_idx,valid_idx) in enumerate(kf.split(train_dataset)):\n    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n    train_dataLoader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_subsampler)\n    valid_dataLoader = DataLoader(train_dataset, batch_size=batch_size, sampler=valid_subsampler)\n    best_acc = 0\n    model = MainClassifier(batch_size).to(device)\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, betas=(0.9, 0.98), lr=2e-5, eps=1e-8)\n    model.zero_grad()\n    print(f'------------fold no---------{fold + 1}----------------------')\n    for epoch_i in range(0, epochs):\n        # model.train(False)\n        model.train()\n        total_loss = 0\n        train_accuracy = 0\n        nb_train_steps = 0\n        for batch in tqdm(train_dataLoader):\n            batch = tuple(t.to(device) for t in batch)\n            sen, att, tok, label = batch\n#             s1, m1, s2, m2, label = batch\n            outputs = model(sen, att, tok)\n#             outputs = model(s1, m1, s2, m2)\n#             outputs = model(s1, s2, m1, m2)\n            # outputs = Arcface(outputs, label)\n            # outputs = sigmoid(outputs)\n            # loss = cal_mse(outputs, label)\n#             loss = criterion(outputs.to(torch.float32), label.unsqueeze(-1).to(torch.float32))\n            loss = criterion(outputs.to(torch.float32), label.to(torch.int64))\n            total_loss += loss.item()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            optimizer.zero_grad()\n            logits = outputs\n#             logits = logits.detach().cpu().numpy()\n#             label = label.unsqueeze(-1).to('cpu').numpy()\n            tmp_train_accuracy = cal_accuracy(logits, label)\n            train_accuracy += tmp_train_accuracy\n            nb_train_steps += 1\n        avg_train_loss = total_loss / len(train_dataLoader)\n        print(\"\")\n        print(epoch_i + 1, \"  Average training loss: {0:.4f}\".format(avg_train_loss))\n        print(\"  Accuracy: {0:.4f}\".format(train_accuracy/(nb_train_steps)))\n        model.eval()\n        eval_loss, eval_accuracy = 0, 0\n        nb_eval_steps, nb_eval_examples = 0, 0\n        valid_loss = 0\n        for batch in tqdm(valid_dataLoader):\n            batch = tuple(t.to(device) for t in batch)\n#             s1, m1, s2, m2, label = batch\n            sen, att, tok, label = batch\n            with torch.no_grad():     \n#                 outputs = model(s1, m1, s2, m2)\n                outputs = model(sen, att, tok)\n            # loss = cal_mse(outputs, label)\n            # outputs = Arcface(outputs, label)\n            # outputs = sigmoid(outputs)\n            # print(outputs)\n#             loss = criterion(outputs.to(torch.float32), label.to(torch.float32))\n#             loss = criterion(outputs.to(torch.float32), label.unsqueeze(-1).to(torch.float32))\n            loss = criterion(outputs.to(torch.float32), label.to(torch.int64))\n            valid_loss += loss.item()\n            logits = outputs\n#             logits = logits.detach().cpu().numpy()\n#             label = label.unsqueeze(-1).to('cpu').numpy()\n            tmp_eval_accuracy = cal_accuracy(logits, label)\n            eval_accuracy += tmp_eval_accuracy\n            nb_eval_steps += 1\n        avg_valid_loss = valid_loss / len(valid_dataLoader)\n        valid_accuracy = eval_accuracy/(nb_eval_steps)\n#         if avg_valid_loss <= best_loss:\n        if best_acc < valid_accuracy:\n            best_acc = valid_accuracy\n#             best_loss = avg_valid_loss\n            torch.save(model, f'/kaggle/working/model{fold + 1}')\n            print(f'model{fold + 1} saved')\n        print(epoch_i + 1, \"  Average valid loss: {0:.4f}\".format(avg_valid_loss))\n        print(\"  Accuracy: {0:.4f}\".format(valid_accuracy))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T20:42:31.279981Z","iopub.execute_input":"2022-02-07T20:42:31.28039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sen = test['text_sum'].apply(encoding1)\natt = test['text_sum'].apply(encoding2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = []\nfor sen1, sen2 in zip(test.premise_, test.hypothesis_):\n    result.append(make_token(sen1, sen2))\ntest['token_type'] = result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sen = sen.apply(padding)\natt = att.apply(padding)\ntok = test['token_type'].apply(padding)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = torch.tensor(sen)\natt_mask = torch.tensor(att)\ntoken_type = torch.tensor(tok)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TensorDataset(input_ids, att_mask, token_type)\n# test_dataset = TensorDataset(input_ids1, att_mask1, input_ids2, att_mask2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=32","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataLoader = DataLoader(test_dataset, batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds = 5\npred = []\nfor i in range(folds) : \n    model = torch.load(f'/kaggle/working/model{i + 1}')\n    model.eval()\n    result = []\n    for batch in tqdm(test_dataLoader):\n        batch = tuple(t.to(device) for t in batch)\n#         s1, m1, s2, m2 = batch\n        sen, mask, tok = batch\n        with torch.no_grad():     \n            outputs = model(sen, mask , tok)\n        result.extend(outputs)    \n    pred.append(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = []\nfor pred1, pred2, pred3, pred4, pred5 in zip(pred[0], pred[1], pred[2], pred[3], pred[4]):\n    output.append(int(torch.argmax(pred1 + pred2 + pred3 + pred4 + pred5)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\n\ndt_now = datetime.datetime.now()\nprint(dt_now)\n# 2020-09-02 15:13:29.383069\n\n# 날짜만 취득\nfname = str(dt_now.date())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out = [list(label_dict.keys())[_] for _ in output]\n\nsubmission[\"label\"] = out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fname","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(f'/kaggle/working/'+ fname + \"_1\" + \".csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}