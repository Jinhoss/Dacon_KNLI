{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom transformers import BertModel, AdamW, BertTokenizer, RobertaTokenizer, RobertaModel, AutoTokenizer, AutoModel\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler\nfrom sklearn.model_selection import train_test_split\nimport random\nimport os\nfrom tqdm import tqdm\nfrom transformers import get_linear_schedule_with_warmup\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nimport math\nfrom torch.optim import Adam\nfrom sklearn.model_selection import KFold\nimport urllib.request\nfrom typing import List\nfrom functools import partial\nimport torchmetrics\n# import torch_xla\n# import torch_xla.core.xla_model as xm\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-08T22:29:43.35016Z","iopub.execute_input":"2022-02-08T22:29:43.350542Z","iopub.status.idle":"2022-02-08T22:29:52.373575Z","shell.execute_reply.started":"2022-02-08T22:29:43.350454Z","shell.execute_reply":"2022-02-08T22:29:52.372661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_num = 22\nrandom.seed(seed_num)\nnp.random.seed(seed_num)\ntorch.manual_seed(seed_num)\ntorch.cuda.manual_seed_all(seed_num)\nkf = KFold(n_splits=5, random_state=seed_num, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T22:29:52.375794Z","iopub.execute_input":"2022-02-08T22:29:52.376303Z","iopub.status.idle":"2022-02-08T22:29:52.385458Z","shell.execute_reply.started":"2022-02-08T22:29:52.376239Z","shell.execute_reply":"2022-02-08T22:29:52.384494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():    \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\nelse:\n    device = torch.device(\"cpu\")\n    print('No GPU available, using the CPU instead.')\n# device = xm.xla_device()\n# torch.set_default_tensor_type('torch.FloatTensor')","metadata":{"execution":{"iopub.status.busy":"2022-02-08T22:29:52.388019Z","iopub.execute_input":"2022-02-08T22:29:52.388363Z","iopub.status.idle":"2022-02-08T22:29:52.404665Z","shell.execute_reply.started":"2022-02-08T22:29:52.388329Z","shell.execute_reply":"2022-02-08T22:29:52.403857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = pd.read_csv('/kaggle/input/kor-nli/dacon/open/train_data.csv')\ntrain = pd.read_csv('/kaggle/input/add-train/update_train.csv')\ntest = pd.read_csv('/kaggle/input/kor-nli/dacon/open/test_data.csv')\nsubmission = pd.read_csv('/kaggle/input/kor-nli/dacon/open/sample_submission.csv')\n# trans = pd.read_csv('/kaggle/input/translate/trans_info.csv')\n# softlabel = pd.read_csv('/kaggle/input/softlabel/softlabel.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-08T22:29:52.406026Z","iopub.execute_input":"2022-02-08T22:29:52.406534Z","iopub.status.idle":"2022-02-08T22:29:52.66648Z","shell.execute_reply.started":"2022-02-08T22:29:52.406481Z","shell.execute_reply":"2022-02-08T22:29:52.665546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SNLIDataset(Dataset):\n\n    def __init__(self, data, is_train=True):\n        super().__init__()\n        self.max_length = 70\n        self.label_dict = {\"entailment\" : 0, \"contradiction\" : 1, \"neutral\" : 2}\n        self.data = data\n        self.tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n        self.is_train = is_train\n        \n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        if self.is_train:\n            sentence_1, sentence_2, label = self.data['premise'][idx], self.data['hypothesis'][idx], self.data['label'][idx]\n            label = self.label_dict[label]\n            label = torch.LongTensor([label])\n        else:\n            sentence_1, sentence_2 = self.data['premise'][idx], self.data['hypothesis'][idx]\n        # remove .\n        sentence_1_input_ids = self.tokenizer.encode(sentence_1, add_special_tokens=False)\n        sentence_2_input_ids = self.tokenizer.encode(sentence_2, add_special_tokens=False)\n        input_ids = sentence_1_input_ids + [2] + sentence_2_input_ids\n        if len(input_ids) > self.max_length - 2:\n            input_ids = input_ids[:self.max_length - 2]\n        # convert list to tensor\n        length = torch.LongTensor([len(input_ids) + 2])\n        input_ids = torch.LongTensor([0] + input_ids + [2])\n        if self.is_train:\n            return input_ids, label, length\n        else:\n            return input_ids, length","metadata":{"execution":{"iopub.status.busy":"2022-02-08T22:29:52.669017Z","iopub.execute_input":"2022-02-08T22:29:52.669298Z","iopub.status.idle":"2022-02-08T22:29:52.682Z","shell.execute_reply.started":"2022-02-08T22:29:52.669241Z","shell.execute_reply":"2022-02-08T22:29:52.681015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\n# train_dataset = TensorDataset(input_ids1, att_mask1, input_ids2, att_mask2, label)\n# train_dataset = TensorDataset(input_ids, att_mask, token_type, label)\n# train_dataset = TensorDataset(input_ids, att_mask, token_type, label, soft)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T22:29:52.683285Z","iopub.execute_input":"2022-02-08T22:29:52.683536Z","iopub.status.idle":"2022-02-08T22:29:52.69673Z","shell.execute_reply.started":"2022-02-08T22:29:52.683507Z","shell.execute_reply":"2022-02-08T22:29:52.695993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = SNLIDataset(train)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T22:29:52.698158Z","iopub.execute_input":"2022-02-08T22:29:52.698666Z","iopub.status.idle":"2022-02-08T22:29:58.861616Z","shell.execute_reply.started":"2022-02-08T22:29:52.698633Z","shell.execute_reply":"2022-02-08T22:29:58.860694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_to_max_length(batch: List[List[torch.Tensor]], max_len: int = None, fill_values: List[float] = None) -> \\\n    List[torch.Tensor]:\n    \"\"\"\n    pad to maximum length of this batch\n    Args:\n        batch: a batch of samples, each contains a list of field data(Tensor), which shape is [seq_length]\n        max_len: specify max length\n        fill_values: specify filled values of each field\n    Returns:\n        output: list of field batched data, which shape is [batch, max_length]\n    \"\"\"\n    # [batch, num_fields]\n    lengths = np.array([[len(field_data) for field_data in sample] for sample in batch])\n    batch_size, num_fields = lengths.shape\n    fill_values = fill_values or [0.0] * num_fields\n    # [num_fields]\n    max_lengths = lengths.max(axis=0)\n    if max_len:\n        assert max_lengths.max() <= max_len\n        max_lengths = np.ones_like(max_lengths) * max_len\n\n    output = [torch.full([batch_size, max_lengths[field_idx]],\n                         fill_value=fill_values[field_idx],\n                         dtype=batch[0][field_idx].dtype)\n              for field_idx in range(num_fields)]\n    for sample_idx in range(batch_size):\n        for field_idx in range(num_fields):\n            # seq_length\n            data = batch[sample_idx][field_idx]\n            output[field_idx][sample_idx][: data.shape[0]] = data\n    # generate span_index and span_mask\n    max_sentence_length = max_lengths[0]\n    start_indexs = []\n    end_indexs = []\n    for i in range(1, max_sentence_length - 1):\n        for j in range(i, max_sentence_length - 1):\n            # # span大小为10\n            # if j - i > 10:\n            #     continue\n            start_indexs.append(i)\n            end_indexs.append(j)\n    # generate span mask\n    span_masks = []\n    for input_ids, label, length in batch:\n        span_mask = []\n        middle_index = input_ids.tolist().index(2)\n        for start_index, end_index in zip(start_indexs, end_indexs):\n            if 1 <= start_index <= length.item() - 2 and 1 <= end_index <= length.item() - 2 and (\n                start_index > middle_index or end_index < middle_index):\n                span_mask.append(0)\n            else:\n                span_mask.append(1e6)\n        span_masks.append(span_mask)\n    # add to output\n    output.append(torch.LongTensor(start_indexs))\n    output.append(torch.LongTensor(end_indexs))\n    output.append(torch.LongTensor(span_masks))\n    return output  # (input_ids, labels, length, start_indexs, end_indexs, span_masks)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T22:29:58.863143Z","iopub.execute_input":"2022-02-08T22:29:58.863696Z","iopub.status.idle":"2022-02-08T22:29:58.88017Z","shell.execute_reply.started":"2022-02-08T22:29:58.863651Z","shell.execute_reply":"2022-02-08T22:29:58.879453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SICModel(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n\n        self.W_1 = nn.Linear(hidden_size, hidden_size)\n        self.W_2 = nn.Linear(hidden_size, hidden_size)\n        self.W_3 = nn.Linear(hidden_size, hidden_size)\n        self.W_4 = nn.Linear(hidden_size, hidden_size)\n\n    def forward(self, hidden_states, start_indexs, end_indexs):\n        W1_h = self.W_1(hidden_states)  # (bs, length, hidden_size)\n        W2_h = self.W_2(hidden_states)\n        W3_h = self.W_3(hidden_states)\n        W4_h = self.W_4(hidden_states)\n\n        W1_hi_emb = torch.index_select(W1_h, 1, start_indexs)  # (bs, span_num, hidden_size)\n        W2_hj_emb = torch.index_select(W2_h, 1, end_indexs)\n        W3_hi_start_emb = torch.index_select(W3_h, 1, start_indexs)\n        W3_hi_end_emb = torch.index_select(W3_h, 1, end_indexs)\n        W4_hj_start_emb = torch.index_select(W4_h, 1, start_indexs)\n        W4_hj_end_emb = torch.index_select(W4_h, 1, end_indexs)\n\n        # [w1*hi, w2*hj, w3(hi-hj), w4(hi⊗hj)]\n        span = W1_hi_emb + W2_hj_emb + (W3_hi_start_emb - W3_hi_end_emb) + torch.mul(W4_hj_start_emb, W4_hj_end_emb)\n        h_ij = torch.tanh(span)\n        return h_ij","metadata":{"execution":{"iopub.status.busy":"2022-02-08T22:29:58.881626Z","iopub.execute_input":"2022-02-08T22:29:58.881973Z","iopub.status.idle":"2022-02-08T22:29:58.899471Z","shell.execute_reply.started":"2022-02-08T22:29:58.881944Z","shell.execute_reply":"2022-02-08T22:29:58.89859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class InterpretationModel(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.h_t = nn.Linear(hidden_size, 1)\n\n    def forward(self, h_ij, span_masks):\n        o_ij = self.h_t(h_ij).squeeze(-1)  # (ba, span_num)\n        # mask illegal span\n        o_ij = o_ij - span_masks\n        # normalize all a_ij, a_ij sum = 1\n        a_ij = nn.functional.softmax(o_ij, dim=1)\n        # weight average span representation to get H\n        H = (a_ij.unsqueeze(-1) * h_ij).sum(dim=1)  # (bs, hidden_size)\n        return H, a_ij","metadata":{"execution":{"iopub.status.busy":"2022-02-08T22:29:58.900775Z","iopub.execute_input":"2022-02-08T22:29:58.901466Z","iopub.status.idle":"2022-02-08T22:29:58.911619Z","shell.execute_reply.started":"2022-02-08T22:29:58.901421Z","shell.execute_reply":"2022-02-08T22:29:58.910751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ExplainableModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n#         self.bert_config = RobertaConfig.from_pretrained(bert_dir, output_hidden_states=False)\n        self.intermediate = AutoModel.from_pretrained(\"klue/roberta-large\")\n        self.span_info_collect = SICModel(1024)\n        self.interpretation = InterpretationModel(1024)\n        self.output = nn.Linear(1024, 3)\n\n    def forward(self, input_ids, start_indexs, end_indexs, span_masks):\n        # generate mask\n        attention_mask = (input_ids != 1).long()\n        # intermediate layer\n        x= self.intermediate(input_ids, attention_mask=attention_mask)  # output.shape = (bs, length, hidden_size)\n        # span info collecting layer(SIC)\n        h_ij = self.span_info_collect(x.last_hidden_state, start_indexs, end_indexs)\n        # interpretation layer\n        H, a_ij = self.interpretation(h_ij, span_masks)\n        # output layer\n        out = self.output(H)\n        return out, a_ij","metadata":{"execution":{"iopub.status.busy":"2022-02-08T22:29:58.912642Z","iopub.execute_input":"2022-02-08T22:29:58.913197Z","iopub.status.idle":"2022-02-08T22:29:58.928178Z","shell.execute_reply.started":"2022-02-08T22:29:58.913162Z","shell.execute_reply":"2022-02-08T22:29:58.927323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 3\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T22:29:58.929242Z","iopub.execute_input":"2022-02-08T22:29:58.929803Z","iopub.status.idle":"2022-02-08T22:29:58.939684Z","shell.execute_reply.started":"2022-02-08T22:29:58.929766Z","shell.execute_reply":"2022-02-08T22:29:58.93885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def cal_accuracy(preds, labels):\n# #     pred_flat = preds>0.5\n#     pred_flat = np.argmax(preds, axis=0).flatten()\n#     labels_flat = labels\n#     return np.sum(pred_flat == labels_flat) / len(labels_flat)\ntrain_acc = torchmetrics.Accuracy()\ndef cal_accuracy(X,Y):\n    predict_scores = F.softmax(X, dim=1)\n    predict_labels = torch.argmax(predict_scores, dim=-1)\n    acc = train_acc(predict_labels.to('cpu'), y.cpu())\n    return train_acc\n#     max_vals, max_indices = torch.max(X, 1)\n#     train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n#     return train_acc","metadata":{"execution":{"iopub.status.busy":"2022-02-08T22:31:48.675355Z","iopub.execute_input":"2022-02-08T22:31:48.67564Z","iopub.status.idle":"2022-02-08T22:31:50.571762Z","shell.execute_reply.started":"2022-02-08T22:31:48.675611Z","shell.execute_reply":"2022-02-08T22:31:50.570805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## new best 0.87\n\nfor fold,(train_idx,valid_idx) in enumerate(kf.split(train_dataset)):\n    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n    train_dataLoader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_subsampler, collate_fn=partial(collate_to_max_length, fill_values=[1, 0, 0]))\n    valid_dataLoader = DataLoader(train_dataset, batch_size=batch_size, sampler=valid_subsampler, collate_fn=partial(collate_to_max_length, fill_values=[1, 0, 0]))\n    best_acc = 0\n#     model = MainClassifier(batch_size).to(device)\n    model = ExplainableModel().to(device)\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, betas=(0.9, 0.98), lr=2e-5, eps=1e-8)\n    model.zero_grad()\n    print(f'------------fold no---------{fold + 1}----------------------')\n    for epoch_i in range(0, epochs):\n        # model.train(False)\n        model.train()\n        total_loss = 0\n        train_accuracy = 0\n        nb_train_steps = 0\n        for batch in tqdm(train_dataLoader):\n            batch = tuple(t.to(device) for t in batch)\n# (input_ids, labels, length, start_indexs, end_indexs, span_masks)\n            sen, label, length, start, end, span = batch\n#             s1, m1, s2, m2, label = batch\n            outputs, a_ij = model(sen, start, end, span)\n            y = label.view(-1)\n#             outputs = model(s1, m1, s2, m2)\n#             outputs = model(s1, s2, m1, m2)\n            # outputs = Arcface(outputs, label)\n            # outputs = sigmoid(outputs)\n            # loss = cal_mse(outputs, label)\n#             loss = criterion(outputs.to(torch.float32), label.unsqueeze(-1).to(torch.float32))\n            ce_loss = criterion(outputs, y)\n#             reg_loss = 1.0 * a_ij.pow(2).sum(dim=1).mean()\n            loss = ce_loss\n            total_loss += loss.item()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            optimizer.zero_grad()\n            logits = outputs\n#             logits = logits.detach().cpu().numpy()\n#             label = label.unsqueeze(-1).to('cpu').numpy()\n            tmp_train_accuracy = cal_accuracy(logits, label.to('cpu').numpy())\n            train_accuracy += tmp_train_accuracy\n            nb_train_steps += 1\n        avg_train_loss = total_loss / len(train_dataLoader)\n        print(\"\")\n        print(epoch_i + 1, \"  Average training loss: {0:.4f}\".format(avg_train_loss))\n        print(\"  Accuracy: {0:.4f}\".format(train_accuracy/(nb_train_steps)))\n        model.eval()\n        eval_loss, eval_accuracy = 0, 0\n        nb_eval_steps, nb_eval_examples = 0, 0\n        valid_loss = 0\n        for batch in tqdm(valid_dataLoader):\n            batch = tuple(t.to(device) for t in batch)\n#             s1, m1, s2, m2, label = batch\n            sen, label, length, start, end, span = batch\n            with torch.no_grad():     \n#                 outputs = model(s1, m1, s2, m2)\n                outputs, a_ij = model(sen, start, end, span)\n            # loss = cal_mse(outputs, label)\n            # outputs = Arcface(outputs, label)\n            # outputs = sigmoid(outputs)\n            # print(outputs)\n#             loss = criterion(outputs.to(torch.float32), label.to(torch.float32))\n#             loss = criterion(outputs.to(torch.float32), label.unsqueeze(-1).to(torch.float32))\n            y = label.view(-1)\n            ce_loss = criterion(outputs, y)\n#             reg_loss = 0.7 * a_ij.pow(2).sum(dim=1).mean()\n            loss = ce_loss\n            valid_loss += ce_loss.item()\n            logits = outputs\n#             logits = logits.detach().cpu().numpy()\n#             label = label.unsqueeze(-1).to('cpu').numpy()\n            tmp_eval_accuracy = cal_accuracy(logits, label.to('cpu').numpy())\n            eval_accuracy += tmp_eval_accuracy\n            nb_eval_steps += 1\n        avg_valid_loss = valid_loss / len(valid_dataLoader)\n        valid_accuracy = eval_accuracy/(nb_eval_steps)\n#         if avg_valid_loss <= best_loss:\n        if best_acc < valid_accuracy:\n            best_acc = valid_accuracy\n#             best_loss = avg_valid_loss\n            torch.save(model, f'/kaggle/working/model{fold + 1}')\n            print(f'model{fold + 1} saved')\n        print(epoch_i + 1, \"  Average valid loss: {0:.4f}\".format(avg_valid_loss))\n        print(\"  Accuracy: {0:.4f}\".format(valid_accuracy))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T22:31:50.574459Z","iopub.execute_input":"2022-02-08T22:31:50.574787Z","iopub.status.idle":"2022-02-08T22:33:08.429729Z","shell.execute_reply.started":"2022-02-08T22:31:50.574745Z","shell.execute_reply":"2022-02-08T22:33:08.426747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = SNLIDataset(test, False)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T22:31:39.973461Z","iopub.status.idle":"2022-02-08T22:31:39.974478Z","shell.execute_reply.started":"2022-02-08T22:31:39.974146Z","shell.execute_reply":"2022-02-08T22:31:39.97418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_test(batch: List[List[torch.Tensor]], max_len: int = None, fill_values: List[float] = None) -> \\\n    List[torch.Tensor]:\n    \"\"\"\n    pad to maximum length of this batch\n    Args:\n        batch: a batch of samples, each contains a list of field data(Tensor), which shape is [seq_length]\n        max_len: specify max length\n        fill_values: specify filled values of each field\n    Returns:\n        output: list of field batched data, which shape is [batch, max_length]\n    \"\"\"\n    # [batch, num_fields]\n    lengths = np.array([[len(field_data) for field_data in sample] for sample in batch])\n    batch_size, num_fields = lengths.shape\n    fill_values = fill_values or [0.0] * num_fields\n    # [num_fields]\n    max_lengths = lengths.max(axis=0)\n    if max_len:\n        assert max_lengths.max() <= max_len\n        max_lengths = np.ones_like(max_lengths) * max_len\n\n    output = [torch.full([batch_size, max_lengths[field_idx]],\n                         fill_value=fill_values[field_idx],\n                         dtype=batch[0][field_idx].dtype)\n              for field_idx in range(num_fields)]\n    for sample_idx in range(batch_size):\n        for field_idx in range(num_fields):\n            # seq_length\n            data = batch[sample_idx][field_idx]\n            output[field_idx][sample_idx][: data.shape[0]] = data\n    # generate span_index and span_mask\n    max_sentence_length = max_lengths[0]\n    start_indexs = []\n    end_indexs = []\n    for i in range(1, max_sentence_length - 1):\n        for j in range(i, max_sentence_length - 1):\n            # # span大小为10\n            # if j - i > 10:\n            #     continue\n            start_indexs.append(i)\n            end_indexs.append(j)\n    # generate span mask\n    span_masks = []\n    for input_ids, length in batch:\n        span_mask = []\n        middle_index = input_ids.tolist().index(2)\n        for start_index, end_index in zip(start_indexs, end_indexs):\n            if 1 <= start_index <= length.item() - 2 and 1 <= end_index <= length.item() - 2 and (\n                start_index > middle_index or end_index < middle_index):\n                span_mask.append(0)\n            else:\n                span_mask.append(1e6)\n        span_masks.append(span_mask)\n    # add to output\n    output.append(torch.LongTensor(start_indexs))\n    output.append(torch.LongTensor(end_indexs))\n    output.append(torch.LongTensor(span_masks))\n    return output  # (input_ids, labels, length, start_indexs, end_indexs, span_masks)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T22:31:39.976686Z","iopub.status.idle":"2022-02-08T22:31:39.977428Z","shell.execute_reply.started":"2022-02-08T22:31:39.977122Z","shell.execute_reply":"2022-02-08T22:31:39.977155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataLoader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=partial(collate_test, fill_values=[1, 0]))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T22:31:39.978582Z","iopub.status.idle":"2022-02-08T22:31:39.979439Z","shell.execute_reply.started":"2022-02-08T22:31:39.979136Z","shell.execute_reply":"2022-02-08T22:31:39.979166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds = 5\npred = []\nfor i in range(folds) : \n    model = torch.load(f'/kaggle/working/model{i + 1}')\n    model.eval()\n    result = []\n    for batch in tqdm(test_dataLoader):\n        batch = tuple(t.to(device) for t in batch)\n#         s1, m1, s2, m2 = batch\n#         sen, mask, tok = batch\n        sen, length, start, end, span = batch\n        with torch.no_grad():     \n            outputs, a_ij = model(sen, start, end, span)\n        result.extend(outputs)    \n    pred.append(result)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T22:31:39.980713Z","iopub.status.idle":"2022-02-08T22:31:39.981816Z","shell.execute_reply.started":"2022-02-08T22:31:39.981535Z","shell.execute_reply":"2022-02-08T22:31:39.981564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = []\nfor pred1, pred2, pred3, pred4, pred5 in zip(pred[0], pred[1], pred[2], pred[3], pred[4]):\n    output.append(int(torch.argmax(pred1 + pred2 + pred3 + pred4 + pred5)))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T22:31:39.983001Z","iopub.status.idle":"2022-02-08T22:31:39.983742Z","shell.execute_reply.started":"2022-02-08T22:31:39.983518Z","shell.execute_reply":"2022-02-08T22:31:39.983544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\n\ndt_now = datetime.datetime.now()\nprint(dt_now)\n# 2020-09-02 15:13:29.383069\n\n# 날짜만 취득\nfname = str(dt_now.date())","metadata":{"execution":{"iopub.status.busy":"2022-02-08T22:31:39.985497Z","iopub.status.idle":"2022-02-08T22:31:39.986243Z","shell.execute_reply.started":"2022-02-08T22:31:39.985965Z","shell.execute_reply":"2022-02-08T22:31:39.985994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_dict = {\"entailment\" : 0, \"contradiction\" : 1, \"neutral\" : 2}\nout = [list(label_dict.keys())[_] for _ in output]\n\nsubmission[\"label\"] = out","metadata":{"execution":{"iopub.status.busy":"2022-02-08T22:31:39.987751Z","iopub.status.idle":"2022-02-08T22:31:39.988489Z","shell.execute_reply.started":"2022-02-08T22:31:39.988188Z","shell.execute_reply":"2022-02-08T22:31:39.988217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2022-02-08T22:31:39.989749Z","iopub.status.idle":"2022-02-08T22:31:39.990624Z","shell.execute_reply.started":"2022-02-08T22:31:39.990316Z","shell.execute_reply":"2022-02-08T22:31:39.990353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(f'/kaggle/working/'+ fname + \"_1\" + \".csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T22:31:39.992052Z","iopub.status.idle":"2022-02-08T22:31:39.992545Z","shell.execute_reply.started":"2022-02-08T22:31:39.99228Z","shell.execute_reply":"2022-02-08T22:31:39.992306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}